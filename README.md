# Data Days For Good Deeds Transcriptions

## Introduction

The Hampden County Registry of Deeds holds a vast collection of historical records dating back to the early 1600s. With approximately 100,000 documents spread across 600 books (each containing around 1,000 documents), these records provide invaluable insights into property ownership, land transactions, and historical boundaries. However, due to the handwritten cursive nature of the documents, inconsistent document delineation, and the lack of a comprehensive indexing system, accessing specific records remains a challenge.

The primary objective of this project is to create a pipeline to digitize and systematically index these historical records to facilitate efficient search and retrieval. This includes:

- **Extracting Key Information**: Identifying grantors, grantees, property descriptions, and geographical references (city, county, streets, NWSE directions).
- **Standardizing Metadata**: Ensuring documents are properly categorized (e.g., deeds, easements, mortgages, liens, plan cards).
- **Improving Searchability**: Creating an indexed system that allows for title searches, boundary dispute research, and historical property ownership tracing.
- **Enhancing Accessibility**: Providing structured access to documents for researchers, legal professionals, and genealogy enthusiasts.

## Table of Contents

- [Installation](#installation)
- [Project Structure](#project-structure)
- [Model Training & Evaluation](#model-training--evaluation)
- [Using the API](#using-the-api)
- [Streamlit Feature](#streamlit-feature)
- [How to Continue](#how-to-continue)

## Installation

To set up the project locally, follow these steps:

1. **Prerequisites**:
   - Python 3.x
   - Jupyter Notebook (for running `.ipynb` files)
   - Access to Boston University’s Shared Computing Cluster (for data)

2. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt

3. **Configure API Credentials**:
  - Obtain an Anthropic Claude API key and set it in Claude-pipeline.ipynb
  - Set up Google Cloud credentials for Document AI and configure them in Google-pipeline.ipynb


## Project Structure


* .github/
* dataset/
  * sample-images/
  * sample-output/
  * DATASETDOC
  * standardized_land_deeds
* notebooks/
  * EDA_MMDDFG_cfgpm.ipynb
  * extract_image_info.ipynb
  * HistoricalDocsClaude-pipeline.ipynb
  * json_to_csv.ipynb
  * microsoft-text-generator.ipynb
  * visualization.ipynb
  * web_scrape.ipynb
* scripts/
  * app.py
  * extract.py
  * mapIndex.py
  * transcriptcompare.py
* LICENSE
* COLLABORATORS
* requirements.txt
* README.md

- **`extract_image_info.ipynb`**: This notebook clarified how to run the project pipeline.

- **`EDA_MMDDFG_cfgpm.ipynb`**  : Exploratory Analysis of the raw imaging files. This includes basic summary statistics.

- **`HistoricalDocsClaude-pipeline.ipynb`** : Generates text from images of known and transcribed historical documents in order to evaluate accruacy of AI models.

- **`json_to_csv.ipynb`**: This notebook transfers the JSON file into csv format. It is also cleaned and standardized.

- **`microsoft-text-generator.ipynb`**: Uses a model generated by microsoft to convert handwritten scripts into computer generated text. However, this model was only able to output 7 words at a time

- **`visualization.ipynb`**  visualizes key metrics of the data. like docs overtime, docs according to the province and county and top 10 grantors and grantees.

- **`web_scrape.ipynb`**: This notebook scrapes data from the government archives website. It then collects it in a dictionary, which was later used to evaluate how our model was performing.

- **`app.py`**: Script to run the streamlit demo.

- **`extract.py`**: Script to extract texts information from raw images.

- **`mapIndex.py`**: Script to bulk index mapping from out csv table.

- **`Transcriptcompare.py`**: Evaluates transcription accuracy by comparing model outputs to known historical transcriptions (e.g., 99% match for Claude on Washington’s 1789 speech). Outputs percentage matches and mismatched words.

## Model Training and Evaluation
  - Since the Hampden County deeds lack transcriptions, we evaluated our models using similar historical documents:

    - Claude Model
      - Achieved 99% match on George Washington’s First Inaugural Speech (1789).
    - Google Model
      - Achieved 43% match on George Washington’s First Inaugural Speech (1789).
    - Microsoft Model
      - Was only able to transcribe the first 7 words in a given text

  - Based on these results, the Claude model was selected for its superior performance on cursive handwriting. Future evaluations will use manually transcribed Hampden County deeds once available.
  

## Using the API

The project leverages two APIs for transcription:

### Anthropic Claude API (in `Claude-pipeline.ipynb`)
- Converts TIF images to PNGs and encodes them in base64.
- Sends the encoded images to Claude with a prompt to extract text and structured data.
- Receives JSON responses containing the transcription and extracted fields.

### Google Cloud Document AI (in `Google-pipeline.ipynb`)
- Sets up a Google Cloud client with credentials.
- Sends TIF images directly to Document AI for OCR.
- Processes the extracted text with spell-checking and grammar correction.

## Streamlit Feature

A Streamlit interface is planned to provide a user-friendly way to upload images, view transcriptions, and search the indexed database. This feature is currently under development.

## How to Continue

Future work includes:

- Expanding the transcription pipeline to additional centuries (e.g., 1600s, 1800s).
- Improving OCR accuracy for faded or damaged documents.
- Developing a Streamlit dashboard for easy access and search functionality.
- Validating transcriptions against ground-truth data from Hampden County once available.